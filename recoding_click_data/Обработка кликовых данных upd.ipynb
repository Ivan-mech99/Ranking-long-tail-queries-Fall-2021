{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1c6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import zipfile\n",
    "import shutil\n",
    "from time import sleep\n",
    "import pickle\n",
    "import tarfile\n",
    "import bz2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613faec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = tarfile.open(\"2017.tar\")\n",
    "#ids=0\n",
    "#for filename in tqdm(t.getnames()):\n",
    "#    if filename!='2017' and filename!='2017/_SUCCESS':\n",
    "#        f=t.extractfile(filename)\n",
    "#        source_file = bz2.BZ2File(f, \"r\")\n",
    "#        for line in source_file:\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a690ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url_id.pickle', 'rb') as file:\n",
    "    url_id = pickle.load(file)\n",
    "with open('query_dict/querry_id.pickle', 'rb') as file:\n",
    "    querry_id = pickle.load(file)\n",
    "with open('query_dict/enlarged_querrys.pickle', 'rb') as file:\n",
    "    enlarged_querrys = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a24fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def http_cut(ex_str):\n",
    "    ex_str = ex_str.replace('http://',\"\")\n",
    "    ex_str = ex_str.replace('https://',\"\")\n",
    "    ex_str = ex_str.replace('www.',\"\")\n",
    "    if len(ex_str)>0:\n",
    "        if ex_str[-1]=='/':\n",
    "            ex_str=ex_str[:-1]\n",
    "    return ex_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9e6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_id_std = dict()\n",
    "id_url_std = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068745f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582167/582167 [00:01<00:00, 338849.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(url_id):\n",
    "    new_key = http_cut(key)\n",
    "    url_id_std[new_key] = url_id[key]\n",
    "    id_url_std[url_id[key]] = new_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a420e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url_id_std.pickle', 'wb') as file:\n",
    "    pickle.dump(url_id_std, file)\n",
    "with open('id_url_std.pickle', 'wb') as file:\n",
    "    pickle.dump(id_url_std, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac3aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc666e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 649/1902 [6:18:28<28:34:28, 82.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648 2017/part-m-01693.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 718/1902 [7:08:27<10:38:11, 32.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717 2017/part-m-01695.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 818/1902 [8:04:11<6:18:20, 20.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816 2017/part-m-00001.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 834/1902 [8:07:29<2:46:18,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831 2017/part-m-00003.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1191/1902 [11:04:01<4:20:48, 22.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1189 2017/part-m-01697.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1290/1902 [11:57:51<4:09:10, 24.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288 2017/part-m-01698.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1531/1902 [13:30:01<6:15:18, 60.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1529 2017/part-m-00002.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1578/1902 [13:52:35<1:26:56, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576 2017/part-m-01696.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 1704/1902 [14:32:00<1:04:30, 19.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1702 2017/part-m-01694.bz2 no relevant info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1902/1902 [15:35:11<00:00, 29.50s/it]  \n"
     ]
    }
   ],
   "source": [
    "min_timestamp = 1483218001000\n",
    "raw_data = []\n",
    "idx = -1\n",
    "str_index = -1\n",
    "t = tarfile.open(\"2017.tar\")\n",
    "for filename in tqdm(t.getnames()):\n",
    "    #try:\n",
    "    if filename!='2017' and filename!='2017/_SUCCESS':\n",
    "        f=t.extractfile(filename)\n",
    "        source_file = bz2.BZ2File(f, \"r\")\n",
    "        idx+=1\n",
    "        for line in source_file:\n",
    "            flag = False\n",
    "            cur_data = []\n",
    "            a,b,c,d = line.decode('utf-8', errors = 'ignore').strip('\\n').split('\\t')\n",
    "            tmp = a.split('@')\n",
    "            querry = tmp[0]\n",
    "            if querry in enlarged_querrys:\n",
    "                cur_data.append(enlarged_querrys[querry])\n",
    "                flag = True\n",
    "            else:\n",
    "                cur_data.append(-1)\n",
    "            geo = int(tmp[1])\n",
    "            cur_data.append(geo)\n",
    "            shown_urls = b.split(',')\n",
    "            shown_urls = [http_cut(word) for word in shown_urls]\n",
    "            url_list = []\n",
    "            loc_bad_dict = dict()\n",
    "            loc_bad_idx=-1\n",
    "            for elem in shown_urls:\n",
    "                if elem in url_id_std:\n",
    "                    url_list.append(url_id_std[elem])\n",
    "                    flag = True\n",
    "                else:\n",
    "                    loc_bad_dict[elem] = loc_bad_idx\n",
    "                    url_list.append(loc_bad_idx)\n",
    "                    loc_bad_idx-=1\n",
    "            cur_data.append(tuple(url_list))\n",
    "            clicked_urls = c.split(',')\n",
    "            clicked_urls = [http_cut(word) for word in clicked_urls]\n",
    "            url_list = []\n",
    "            for elem in clicked_urls:\n",
    "                if elem in url_id_std:\n",
    "                    url_list.append(url_id_std[elem])\n",
    "                else:\n",
    "                    url_list.append(loc_bad_dict[elem])\n",
    "            cur_data.append(tuple(url_list))\n",
    "            timestamps = [int(el) for el in d.split(',')]\n",
    "            cur_data.append(tuple(timestamps))\n",
    "            if flag:\n",
    "                str_index+=1\n",
    "                cur_data.append(str_index)\n",
    "                raw_data.append(tuple(cur_data))\n",
    "        if raw_data==[]:\n",
    "            print(idx, filename, 'no relevant info')\n",
    "        with open('raw_{}.pickle'.format(idx), 'wb') as file:\n",
    "            pickle.dump(tuple(raw_data), file)\n",
    "        zip_obj= zipfile.ZipFile(\"raw_{}.zip\".format(idx),\"w\", compression=zipfile.ZIP_DEFLATED)\n",
    "        zip_obj.write(\"raw_{}.pickle\".format(idx))\n",
    "        zip_obj.close()\n",
    "        shutil.copy2(\"raw_{}.zip\".format(idx), 'raw_click_data_upd/'+\"raw_{}.zip\".format(idx))\n",
    "        try:\n",
    "            os.remove(\"raw_{}.zip\".format(idx))\n",
    "            os.remove(\"raw_{}.pickle\".format(idx))\n",
    "        except PermissionError:\n",
    "            sleep(3)\n",
    "            os.remove(\"raw_{}.zip\".format(idx))\n",
    "            os.remove(\"raw_{}.pickle\".format(idx))\n",
    "        raw_data = []\n",
    "    #except:\n",
    "        #print(filename)\n",
    "        #continue\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539b474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
